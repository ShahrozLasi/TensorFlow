{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataSceProject_DrYali.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YfVwAiwr8GYr","colab_type":"code","outputId":"d5c31a57-42c8-41fd-f3ed-32aa6e463114","executionInfo":{"status":"ok","timestamp":1584109977506,"user_tz":-60,"elapsed":214196,"user":{"displayName":"shahroz lasi","photoUrl":"","userId":"12196692998240518202"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["# Mounting a new session\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BO9Rkb9utWuI","colab_type":"code","colab":{}},"source":["# get the data from the zip file\n","!unzip -uq \"/content/drive/My Drive/DATA/auth_day_9_labelled.zip\" -d \"/content/drive/My Drive/DATA\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nl1VJeWQGSCM","colab_type":"code","colab":{}},"source":["import numpy as np\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","import tensorflow as tf\n","# if tf.__version__ < \"2.0\":\n","#   !pip install --upgrade tensorflow\n","\n","from keras.models import Model, load_model\n","from keras.layers import LSTM, Dense, Dense, Dropout, GaussianNoise\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.layers import LSTM\n","from keras.layers import Input\n","from keras import regularizers\n","from sklearn.preprocessing import LabelEncoder\n","from keras.layers.normalization import BatchNormalization\n","from keras.optimizers import adagrad, RMSprop, Adam\n","from scipy.stats import lognorm\n","import datetime\n","\n","from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n","                              roc_curve, recall_score, classification_report, f1_score,\n","                              precision_recall_fscore_support)\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jluO-XCHReZa","colab_type":"code","colab":{}},"source":["class LSTM_autoencoder:\n","    \"\"\"\n","    This function implements lstm autoencoder. It trains the model on normal log-lines and\n","    evaluates the model in the final phase\n","    \n","    \"\"\"\n","    \n","    \n","    def __init__(self):\n","        pass\n","    \n","        \n","    def split_user_domain(self, dataset):\n","        \"\"\"\n","        This function is for data cleaning - removing missing values\n","        \n","        \"\"\"\n","        dataset['source_user'] = dataset['source_user@domain'].str.split('@').str[0]\n","        dataset['source_user'] = np.where(dataset['source_user'].str.contains('$'), dataset['source_user'].str.split('$').str[0],\n","                                    dataset['source_user'])\n","        dataset['source_domain'] = dataset['source_user@domain'].str.split('@').str[1]\n","\n","        dataset['destination_user'] = dataset['destination_user@domain'].str.split('@').str[0]\n","        dataset['destination_user'] = np.where(dataset['destination_user'].str.contains('$'), \n","                                               dataset['destination_user'].str.split('$').str[0],\n","                                         dataset['destination_user'])\n","        dataset['destination_domain'] = dataset['destination_user@domain'].str.split('@').str[1]\n","        \n","        self.dataset = dataset\n","    \n","    def drop_columns(self):\n","        \"\"\" This function drops the irrelevant columns. \"\"\"\n","               \n","        self.dataset = self.dataset.drop(['time', 'source_user@domain', 'destination_user@domain'], axis = 1)\n","        \n","#         return self.dataset\n","\n","    def transform_data(self):\n","        \"\"\" This function encodes each column based on the values. \"\"\"\n","        \n","        from sklearn.preprocessing import LabelEncoder\n","        lb = LabelEncoder()\n","        for cols in self.dataset.columns:\n","            self.dataset[cols] = lb.fit_transform(self.dataset[cols])\n","\n","    def normalize(self):\n","        \"\"\" This function normalizes each column based on the range of the column. \"\"\"\n","        from sklearn.preprocessing import StandardScaler\n","        from sklearn.preprocessing import MinMaxScaler\n","\n","        for cols in self.dataset.columns:\n","            self.dataset[cols] = MinMaxScaler(feature_range=(0,1)).fit_transform(self.dataset[cols].values.reshape(-1,1))\n","        \n","    def sep_x_y(self):\n","        \"\"\" this function separates the \"\"\"\n","        features = list()\n","        for cols in self.dataset.columns:\n","            if cols != 'Malignant/Benign':\n","                features.append(cols)\n","        self.x = self.dataset[features]\n","        self.y = self.dataset['Malignant/Benign']\n","        \n","#         return self.x,self.y\n","\n","    def df_to_array(self):\n","        self.x = self.x.values\n","        \n","#         return self.x\n","    \n","    def shape_to_3dim(self):\n","        \"\"\" This function changes the shape of the array for the LSTM -autoencoder.\n","            This has to be done for test set as well.\n","            Only for features (i.e x-values)\n","        \"\"\"\n","        self.x = self.x.reshape(self.x.shape[0], 1, self.x.shape[1])\n","#         return self.x\n","    def pred(self):\n","        self.predictions = self.autoencoder.predict(self.x)\n","#         return self.predictions\n","    \n","    \n","    def fit(self, nb_epoch = 100, batch_size = 512):\n","            ## Creating the Model\n","            repeat_vector = self.x.shape[1]\n","            output_dim = self.x.shape[2] # number of features\n","            learning_rate = 1e-6\n","            max_len = 120\n","            num_chars = 128\n","            layers = 128\n","            autoencoder = Sequential()\n","            # Encoder\n","            # autoencoder.add(LSTM(layers, return_sequences=True, input_shape=(max_len, num_chars),\n","            #                      dropout = 0.5))\n","            autoencoder.add(LSTM(layers, return_sequences=True))\n","            autoencoder.add(GaussianNoise(0.8))\n","            autoencoder.add(BatchNormalization())\n","            autoencoder.add(Dropout(0.5))\n","            # autoencoder.add(RepeatVector(repeat_vector))\n","\n","            autoencoder.add(Dense(128, activation='relu'))\n","            autoencoder.add(BatchNormalization())\n","            autoencoder.add(Dense(output_dim, activation='softmax'))\n","            # autoencoder.add(TimeDistributed(Dense(output_dim, activation='softmax')))\n","\n","            # autoencoder.add(LSTM(256, activation='relu', activity_regularizer=regularizers.l1(learning_rate),return_sequences=True))\n","            # autoencoder.add(LSTM(128, activation='sigmoid', activity_regularizer=regularizers.l2(learning_rate),return_sequences=False))\n","            # autoencoder.add(Dense(64, activation = 'softmax'))\n","            # autoencoder.add(Dropout(0.5))\n","            # autoencoder.add(GaussianNoise(0.8))\n","            # autoencoder.add(BatchNormalization())\n","            # autoencoder.add(Dense(32, activation = 'relu'))\n","            # autoencoder.add(Dropout(0.5))\n","            # autoencoder.add(Dense(16, activation='sigmoid'))\n","            # autoencoder.add(RepeatVector(repeat_vector))\n","\n","            # # Decoder\n","            # autoencoder.add(Dense(16, activation='sigmoid'))\n","            # autoencoder.add(Dense(32, activation='relu'))\n","            # autoencoder.add(Dense(64, activation= 'sigmoid'))\n","            # autoencoder.add(LSTM(128, activation='sigmoid', activity_regularizer=regularizers.l2(learning_rate),return_sequences=True))\n","            # autoencoder.add(LSTM(256, activation='relu', activity_regularizer=regularizers.l1(learning_rate),return_sequences=True))\n","            # autoencoder.add(TimeDistributed(Dense(output_dim)))\n","\n","            # Model Compiler\n","            # opt = adagrad(lr = 1e-3, decay = 1e-5)\n","            # opt = RMSprop(lr = 0.001, epsilon = 1e-06)\n","            opt = Adam(lr = 0.001, epsilon = 1e-06)\n","\n","            autoencoder.compile(metrics=['accuracy'],\n","                                loss='mean_squared_error',\n","                                optimizer= opt)\n","            autoencoder.fit(self.x, self.x, epochs=nb_epoch, \n","                                      batch_size = batch_size, shuffle = True ,verbose = 0)\n","            self.autoencoder = autoencoder\n","\n","    \n","\n","\n","    def prediction_results(self):\n","            \n","            self.y = self.y.reset_index()['Malignant/Benign']\n","            self.x = self.x.reshape(self.x.shape[0], self.x.shape[2])\n","            self.predictions = self.predictions.reshape(self.predictions.shape[0], self.predictions.shape[2])\n","\n","            mse = np.mean(np.power(self.x - self.predictions, 2), axis=1)\n","\n","            error_df = pd.DataFrame({'reconstruction_error': mse,\n","                                'true_class': self.y})\n","            \n","            fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n","            roc_auc = auc(fpr, tpr)\n","            \n","            threshold = min(thresholds)\n","            LABELS = [\"Normal\", \"MALIGNANT\"]\n","            y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n","            conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n","            \n","            ##Performance check of the model\n","\n","            TP = FN = FP = TN = 0\n","\n","            for j in range(len(self.y)):\n","                if self.y[j]== 1 and y_pred[j] == 1:\n","                    TP = TP+1\n","                elif self.y[j]== 1 and y_pred[j] == 0:\n","                    FN = FN+1\n","                elif self.y[j]== 0 and y_pred[j] == 1:\n","                    FP = FP+1\n","                elif self.y[j] == 0 and y_pred[j] == 0:\n","                    TN = TN +1\n","            # print (TP,  FN,  FP,  TN)\n","\n","            TPR = TP/(TP+FN)\n","            FPR = FP/(FP+TN)\n","            TNR = TN/(TN+FP)\n","            FNR = FN/(FN+TP)\n","\n","\n","            accuracy = (TP+TN)/(TP+FN+FP+TN)\n","            sensitivity = TP/(TP+FN)\n","            specificity = TN/(TN+FP)\n","            precision = TP/(TP+FP) \n","\n","            print (\"TPR: \", np.round(TPR, 4), \n","                  \"; FPR: \", np.round(FPR, 4),\n","                  \"; TNR: \", np.round(TNR, 4),\n","                  \"; FNR: \", np.round(FNR, 4))\n","\n","            print (\"Accuracy: \", np.round(accuracy, 4), \n","                  \"; Sensitivity: \", np.round(sensitivity, 4),\n","                  \"; Specificity: \", np.round(specificity, 4),\n","                  \"; Precision: \", np.round(precision, 4),\n","                  \"; F1-score: \" , np.round(2/((1/precision) + (1/sensitivity)),4))\n","            \n","#             return self.error_df\n","\n","\n","    def create_subsets(self,test_set):\n","\n","            n1, n2, n3 = 500,300,200\n","            q1, q2, q3 = 200,200,200\n","            normal = test_set[test_set['Malignant/Benign'] == 0]\n","            abnormal = test_set[test_set['Malignant/Benign'] == 1]\n","            t1 =  normal[:n1]\n","            t2 =  normal[n1:n1+n2]\n","            t3 =  normal[n2:n2+n3]\n","\n","            t1 = t1.append(abnormal)\n","            t2 = t2.append(abnormal)\n","            t3 = t3.append(abnormal)\n","            \n","#             self.t1, self.t2, self.t3 = t1,t2,t3\n","\n","            return t1, t2, t3\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gw7y4xEKxF4y","colab_type":"code","outputId":"7f6f97ef-024c-4df7-fda3-7d167c1050e9","executionInfo":{"status":"ok","timestamp":1584113380820,"user_tz":-60,"elapsed":18627,"user":{"displayName":"shahroz lasi","photoUrl":"","userId":"12196692998240518202"}},"colab":{"base_uri":"https://localhost:8080/","height":715}},"source":["df_9 = pd.read_csv(\"/content/drive/My Drive/DATA/auth_day_9_labelled.txt\", header = None)\n","test_1 = pd.read_csv(\"/content/drive/My Drive/DATA/Test_day_9.txt\")\n","df_9.columns = [\"time\",\"source_user@domain\",\"destination_user@domain\",\"source_computer\",\"destination_computer\",\n","              \"authentication_type\",\"logon_type\",\"authentication_orientation\",\"success/failure\", \"Malignant/Benign\"]\n","\n","lstm = LSTM_autoencoder()\n","## Using the training data only\n","# Training\n","lstm.split_user_domain(df_9[:1000]) ## change 1000 to more rows\n","lstm.drop_columns()\n","lstm.transform_data()\n","lstm.normalize()\n","lstm.sep_x_y()\n","lstm.df_to_array()\n","lstm.shape_to_3dim()\n","lstm.fit()\n","\n","## Testing\n","## Now use the test set - first with 10,200 mixed observations\n","t1,t2,t3 = lstm.create_subsets(test_1[test_1.columns[1:]])\n","t = [test_1[test_1.columns[1:]], t1,t2,t3,]\n","for test_set in t:\n","    lstm.split_user_domain(test_set)\n","    lstm.drop_columns()\n","    lstm.transform_data()\n","    lstm.normalize()\n","    lstm.sep_x_y()\n","    lstm.df_to_array()\n","    lstm.shape_to_3dim()\n","    lstm.pred()\n","    lstm.prediction_results()"],"execution_count":45,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"stream","text":["TPR:  1.0 ; FPR:  0.9999 ; TNR:  0.0001 ; FNR:  0.0\n","Accuracy:  0.0197 ; Sensitivity:  1.0 ; Specificity:  0.0001 ; Precision:  0.0196 ; F1-score:  0.0385\n","TPR:  1.0 ; FPR:  0.998 ; TNR:  0.002 ; FNR:  0.0\n","Accuracy:  0.2871 ; Sensitivity:  1.0 ; Specificity:  0.002 ; Precision:  0.2861 ; F1-score:  0.4449\n","TPR:  1.0 ; FPR:  0.9967 ; TNR:  0.0033 ; FNR:  0.0\n","Accuracy:  0.402 ; Sensitivity:  1.0 ; Specificity:  0.0033 ; Precision:  0.4008 ; F1-score:  0.5722\n","TPR:  1.0 ; FPR:  0.995 ; TNR:  0.005 ; FNR:  0.0\n","Accuracy:  0.5025 ; Sensitivity:  1.0 ; Specificity:  0.005 ; Precision:  0.5013 ; F1-score:  0.6678\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mEtYpo7c1P2s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":421},"outputId":"12f03144-bac5-443a-b5c5-d5246263f183","executionInfo":{"status":"ok","timestamp":1584113195477,"user_tz":-60,"elapsed":900,"user":{"displayName":"shahroz lasi","photoUrl":"","userId":"12196692998240518202"}}},"source":["lstm.autoencoder.summary()"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Model: \"sequential_12\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_19 (LSTM)               (None, 1, 128)            71168     \n","_________________________________________________________________\n","gaussian_noise_9 (GaussianNo (None, 1, 128)            0         \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 1, 128)            512       \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, 1, 128)            0         \n","_________________________________________________________________\n","dense_27 (Dense)             (None, 1, 128)            16512     \n","_________________________________________________________________\n","batch_normalization_12 (Batc (None, 1, 128)            512       \n","_________________________________________________________________\n","dense_28 (Dense)             (None, 1, 10)             1290      \n","=================================================================\n","Total params: 89,994\n","Trainable params: 89,482\n","Non-trainable params: 512\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"16C74Xthirq9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":421},"outputId":"34a91dac-4006-48b5-a6ec-5b34cf38a237","executionInfo":{"status":"ok","timestamp":1584113862433,"user_tz":-60,"elapsed":591,"user":{"displayName":"shahroz lasi","photoUrl":"","userId":"12196692998240518202"}}},"source":["lstm.autoencoder.summary()"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Model: \"sequential_13\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_20 (LSTM)               (None, 1, 128)            71168     \n","_________________________________________________________________\n","gaussian_noise_10 (GaussianN (None, 1, 128)            0         \n","_________________________________________________________________\n","batch_normalization_13 (Batc (None, 1, 128)            512       \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, 1, 128)            0         \n","_________________________________________________________________\n","dense_29 (Dense)             (None, 1, 128)            16512     \n","_________________________________________________________________\n","batch_normalization_14 (Batc (None, 1, 128)            512       \n","_________________________________________________________________\n","dense_30 (Dense)             (None, 1, 10)             1290      \n","=================================================================\n","Total params: 89,994\n","Trainable params: 89,482\n","Non-trainable params: 512\n","_________________________________________________________________\n"],"name":"stdout"}]}]}